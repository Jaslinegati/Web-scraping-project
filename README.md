# Streamlined Web-Scraping Pipeline for Amazon Data

## Overview

This project involves creating an advanced data pipeline for a web-scraping project that scrapes data from Amazon and stores it in an on-premise SQL database. The project includes an efficient ELT (Extract, Load, Transform) process to ensure the smooth flow of data from source to destination. The project also includes automation scripts to orchestrate various ELT and DDL (Data Definition Language) scripts, resulting in a highly effective and reliable pipeline.

## Technologies Used

Python
BeautifulSoup
SQL

## Getting Started

To get started with the project, follow the steps below:

Clone the project to your local machine.
Install the required Python packages using pip install -r requirements.txt.
Configure the database credentials and other parameters in the configuration file.
Run the run_pipeline.py script to start the pipeline.

## Pipeline Architecture

The pipeline includes the following components:

Web scraper: This component uses BeautifulSoup to scrape data from Amazon and extracts the relevant data fields.
Extract: This component extracts the data from the web scraper and loads it into memory.
Load: This component loads the data into the on-premise SQL database.
Transform: This component transforms the data as per the requirements of the project.
Analysis: This component performs various data analysis tasks on the transformed data.

## Conclusion

This project delivers an efficient and reliable pipeline for web scraping data from Amazon and storing it in an on-premise SQL database. The pipeline includes an efficient ELT process and automation scripts to orchestrate various ETL and DDL scripts. The project utilizes cutting-edge technologies such as BeautifulSoup and SQL to deliver a highly effective pipeline that exceeded the expectations of the team.



